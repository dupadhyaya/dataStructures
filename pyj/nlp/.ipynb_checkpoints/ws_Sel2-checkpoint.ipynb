{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d585bcf-4239-4755-a084-e90fb098ef49",
   "metadata": {},
   "source": [
    "# Web Scraping using Selenium\n",
    "- https://brightdata.com/blog/how-tos/scrape-dynamic-websites-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aef85aad-7dc3-4eb3-8f99-a4032c43c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install selenium\n",
    "#!pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1ff207d-00bd-4989-acd9-ee6f0325eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcee7cb7-3e56-487d-af68-66ccb5194e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0539164a-042d-47d3-b729-51b0e5de36a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL\n",
    "url = \"https://www.youtube.com/@programmingwithmosh/videos\"\n",
    "\n",
    "# load the web page\n",
    "driver.get(url)\n",
    "\n",
    "# set maximum time to load the web page in seconds\n",
    "driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099022aa-62a7-4579-bc27-02b161b57e8c",
   "metadata": {},
   "source": [
    "- Selenium automatically loads the YouTube link in the Chrome browser.\n",
    "- Additionally, a time frame is specified (ie ten seconds) to make sure that the web page is fully loaded (including all HTML elements). This helps you scrape data that is rendered by JavaScript.\n",
    "- Scrape Data Using ID and Tags\n",
    "- One of the benefits of Selenium is that it can extract data using different elements presented on the web page, including the ID and tag.\n",
    "- For instance, you can use either the ID element (ie post-title) or tags (ie h1 and p) to scrape the data:\n",
    "- -<h1 id =\"post-title\">Introduction to data scrapping using Python</h1>\n",
    "- <p>You can use selenium python package to collect data from any dynamic website</p>\n",
    "- Use Webdriver to scrape data that is within the ID identified. To find an HTML element by ID attribute, call the find_element() Selenium method and pass By.ID as the first argument and ID as the second argument.\n",
    "- To collect the video title and video link for each video, you need to use the video-title-link ID attribute. Since you’re going to collect multiple HTML elements with this ID attribute, you’ll need to use the find_elements() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75dbc085-df92-453b-8647-42c03e54a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data that are withing the id of contents\n",
    "contents = driver.find_element(By.ID, \"contents\")\n",
    "\n",
    "#1 Get all the by video tite link using id video-title-link\n",
    "video_elements = contents.find_elements(By.ID, \"video-title-link\")\n",
    "\n",
    "#2 collect title and link for each youtube video\n",
    "titles = []\n",
    "links = []\n",
    "\n",
    "for video in video_elements:\n",
    "\n",
    "    #3 Extract the video title\n",
    "    video_title = video.get_attribute(\"title\")\n",
    "\n",
    "    #4 append the video title\n",
    "    titles.append(video_title)\n",
    "\n",
    "    #5 Extract the video link\n",
    "    video_link = video.get_attribute(\"href\")\n",
    "\n",
    "#6 append the video link\n",
    "links.append(video_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43625a-6c1a-4cd7-946e-f484c20edaa2",
   "metadata": {},
   "source": [
    "- This code performs the following tasks:\n",
    "  - It collects data that is within the ID attribute of contents.\n",
    "  -  It collects all HTML elements that have an ID attribute of video-title-link from the WebElement contents object.\n",
    "  - It creates two lists to append titles and links.\n",
    "  - It extracts the video title using the get_attribute()method and passes the title.\n",
    "  - It appends the video title in the titles list.\n",
    "  - It extracts the video link using the get_atribute() method and passes href as an argument.\n",
    "  - It appends the video link in the links list.\n",
    "- At this point, all the video titles and links will be in two Python lists: titles and links.\n",
    "- - Next, you need to scrape the link of the image that is available on the web page before you click the YouTube video link to watch the video. To scrape this image link, you need to find all the HTML elements by calling the find_elements() Selenium method and passing By.TAG_NAME as the first argument and the name of the tag as the second argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e125aa8b-1637-4cf0-87ac-f747b1916d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Get all the by Tag\n",
    "img_elements = contents.find_elements(By.TAG_NAME, \"img\")\n",
    "\n",
    "#2 collect img link and link for each youtube video\n",
    "img_links = []\n",
    "\n",
    "for img in img_elements:\n",
    "\n",
    "    #3 Extract the img link\n",
    "    img_link = img.get_attribute(\"src\")\n",
    "    if img_link:\n",
    "        #4 append the img link\n",
    "        img_links.append(img_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e1ae95-0367-498a-81e3-7b43fbda4fcd",
   "metadata": {},
   "source": [
    "- This code collects all the HTML elements with the img tag name from the WebElement object called contents. It also creates a list to append the image links and extracts it using the get_attribute() method and passes src as an argument. Finally, it appends the image link to the img_links list.\n",
    "- You can also use the ID and the tag name to scrape more data for each YouTube video. On the web page of the YouTube URL, you should be able to see the number of views and the time published for each video listed on the page. To extract this data, you need to collect all the HTML elements that have an ID of metadata-line and then collect data from the HTML elements with a span tag name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fac0f89-340f-428f-b1bc-9e5059982de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['109K views', '1 day ago'], ['168K views', '7 days ago'], ['34K views', '3 weeks ago'], ['39K views', '1 month ago'], ['83K views', '3 months ago'], ['64K views', '4 months ago'], ['41K views', '4 months ago'], ['135K views', '5 months ago'], ['419K views', '6 months ago'], ['32K views', '6 months ago'], ['2.2M views', '1 year ago'], ['3.5M views', '1 year ago'], ['48K views', '1 year ago'], ['82K views', '1 year ago'], ['3.2M views', '1 year ago'], ['1M views', '1 year ago'], ['402K views', '1 year ago'], ['159K views', '2 years ago'], ['58K views', '2 years ago'], ['67K views', '2 years ago'], ['130K views', '2 years ago'], ['56K views', '2 years ago'], ['81K views', '2 years ago'], ['104K views', '2 years ago'], ['2.2M views', '2 years ago'], ['2.3M views', '2 years ago'], ['45K views', '2 years ago'], ['8.1M views', '3 years ago'], ['1.4M views', '3 years ago'], ['221K views', '3 years ago']]\n"
     ]
    }
   ],
   "source": [
    "#1 find the element with the specific ID you want to scrape\n",
    "meta_data_elements = contents.find_elements(By.ID, 'metadata-line')\n",
    "\n",
    "#2 collect data from span tag\n",
    "meta_data = []\n",
    "\n",
    "for element in meta_data_elements:\n",
    "    #3 collect span HTML element\n",
    "    span_tags = element.find_elements(By.TAG_NAME, 'span')\n",
    "\n",
    "    #4 collect span data\n",
    "    span_data = []\n",
    "    for span in span_tags:\n",
    "        #5 extract data for each span HMTL element.\n",
    "        span_data.append(span.text)\n",
    "    #6 append span data to the list\n",
    "    meta_data.append(span_data)\n",
    "\n",
    "# print out the scraped data.\n",
    "print(meta_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f14d296-00d9-478b-9792-d1567069ca28",
   "metadata": {},
   "source": [
    "- This code block collects all the HTML elements that have an ID attribute of metadata-line from the WebElement contents object and creates a list to append data from the span tag that will have the number of views and the time published.\n",
    "- It also collects all the HTML elements whose tag name is span from the WebElement object called meta_data_elements and creates a list with this span data. Then it extracts the text data from the span HTML element and appends it to the span_data list. Finally, it appends the data from the span_data list to the meta_data.\n",
    "- The data extracted from the span HTML element will look like this:\n",
    "- - Next, you need to create two Python lists and save the number of views and time published separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "435d5473-94c7-4fca-a8c6-d336503aa01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Iterate over the list of lists and collect the first and second item of each sublist\n",
    "views_list = []\n",
    "published_list = []\n",
    "\n",
    "for sublist in meta_data:\n",
    "    #2 append number of views in the views_list\n",
    "    views_list.append(sublist[0])\n",
    "\n",
    "    #3 append time published in the published_list\n",
    "    published_list.append(sublist[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74d9f9d-589e-4ef9-80b2-6db6806e4a23",
   "metadata": {},
   "source": [
    "- Here, you create two Python lists that extract data from meta_data, and you append the number of views for each sublist to view_list and the time published for each sublist to the published_list.\n",
    "- At this point, you’ve scraped the title of the video, the URL of the video page, the URL of the image, the number of views, and the time the video was published. This data can be saved into a pandas DataFrame using the pandas Python package. Use the following code to save the data from the list of titles, links, img_links, views_list, and published_list into the pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa4c37fc-a1a2-42c4-b8db-406a18ab78c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in pandas dataFrame\n",
    "data = pd.DataFrame(\n",
    "list(zip(titles, links, img_links, views_list, published_list)),\n",
    "columns=['Title', 'Link', 'Img_Link', 'Views', 'Published'])\n",
    "\n",
    "# show the top 10 rows\n",
    "#data.head(10)\n",
    "\n",
    "# export data into a csv file.\n",
    "#data.to_csv(\"../data/youtube_data.csv\",index=False)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25f6b5b1-b914-45fa-8375-829846273180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Img_Link</th>\n",
       "      <th>Views</th>\n",
       "      <th>Published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is Devin AI the end (or future) of coding?!</td>\n",
       "      <td>https://www.youtube.com/watch?v=Nb0btdq1164</td>\n",
       "      <td>https://i.ytimg.com/vi/XKkoVpupYdw/hqdefault_c...</td>\n",
       "      <td>109K views</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Title  \\\n",
       "0  Is Devin AI the end (or future) of coding?!   \n",
       "\n",
       "                                          Link  \\\n",
       "0  https://www.youtube.com/watch?v=Nb0btdq1164   \n",
       "\n",
       "                                            Img_Link       Views  Published  \n",
       "0  https://i.ytimg.com/vi/XKkoVpupYdw/hqdefault_c...  109K views  1 day ago  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a39b8-5423-4681-92c9-1aabc592df11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
